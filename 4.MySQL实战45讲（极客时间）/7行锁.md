# 7行锁

* 如何减少行锁对性能的影响？

* 行锁是在存储引擎层面实现的，所以MyISAM表没有实现行锁，在更新一行数据时，需要锁住全表。大大降低了业务并发度，这就是innodb代理它的一个重要原因。

## 7.1 一些概念

### 7.1.1 两阶段锁

* 如下图：

![](./7_1.jpeg)

* 事务b会被阻塞住，知道事务A提交。这就是事务A持有这两个记录的行锁，在commit时释放。
* 所以在innode事务中，行锁是在需要的时候加，但是是在事务结束时才释放，这就是两阶段锁协议。所以说事务中需要锁多个行时，要把最可能造成锁冲突，最有可能影响并发度的锁尽量往后放。
* 一个例子：

  * 假设你负责实现一个电影票在线交易业务，顾客 A 要在影院 B 购买电影票。我们简化一点，这个业务需要涉及到以下操作：

    * 从顾客 A 账户余额中扣除电影票价；

    * 给影院 B 的账户余额增加这张电影票价；

    * 记录一条交易日志。

* 也就是说，要完成这个交易，我们需要 update 两条记录，并 insert 一条记录。当然，为了保证交易的原子性，我们要把这三个操作放在一个事务中。那么，你会怎样安排这三个语句在事务中的顺序呢？
* 试想如果同时有另外一个顾客 C 要在影院 B 买票，那么这两个事务冲突的部分就是语句 2 了。因为它们要更新同一个影院账户的余额，需要修改同一行数据。
* 根据两阶段锁协议，不论你怎样安排语句顺序，所有的操作需要的行锁都是在事务提交的时候才释放的。所以，如果你把语句 2 安排在最后，比如按照 3、1、2 这样的顺序，那么影院账户余额这一行的锁时间就最少。这就最大程度地减少了事务之间的锁等待，提升了并发度。

### 7.1.2 死锁和死锁检测

* 如果这个影院做活动，可以低价预售一年内所有的电影票，而且这个活动只做一天。于是在活动时间开始的时候，你的 MySQL 就挂了。你登上服务器一看，CPU 消耗接近 100%，但整个数据库每秒就执行不到 100 个事务。这是什么原因呢？
* 对于这种死锁innodb有两个处理方式
  * 等待死锁超时：innodb_lock_wait_timeout 默认50s，但是这个时间太长，业务难以接受。但是又不能设置的太短，假如只是简单的锁等待，会误杀很多事物
  * 死锁检测：innodb_deadlock_detect。每个新来的事务被锁住时，都要去检查是不是自己的加入导致死锁，也就是要看所依赖的线程有没有被别人锁住。这是个o(n)的造作，假设有1000个并发线程，检查就是100w级别，会大量占用CPU，却执行不了几个事务。这就是热点行带来的问题。
    * 一个思路是临时关闭死锁检测=》可能出现大量超时影响业务
    * 做并发控制
      * 客户端做：单个客户端限制一定的并发度，但是客户端增多还是有问题
      * 服务端做：对于形同行更新，在进入引擎前排队，就不会有大量死锁
    * 但是如果没有人能来改造源码，可以在业务上改造
      * 上边的例子，将影院的账户改成10个逻辑账户，后边业务请求底时再合并

## 一些问题答疑

* 行锁是给予索引记录实现的，如果update的列没有索引，会锁住全表？

  * 是的，会锁住。

* 是每条事务都要做死锁检测吗？

  * 如果他要加锁的行上有锁，才检测，如果如下事务：A等B，D等C，来了新事物E，发现E要等D，那么E要判断跟D、C是否会产生死锁。这个检测不管A、B
