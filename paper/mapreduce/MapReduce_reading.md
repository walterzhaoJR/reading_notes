# MapReduce 阅读

## 笔记
### 1.简介

* mapreduce是一个编程模型。用户指定一个map函数处理输入的KV对，产生中间结果的KV对集合。然后指定一个reduce函数，聚合形同key的value。
* 这种方式编写的程序可以在大规模机器上并行执行。

### 2.简单的实现

* mapruduce的基本工作流程：
  * 用户程序中的MapReduce库首先将输入文件切分为M块，每块的大小从16MB到64MB（用户可通过一个可选参数控制此大小）。然后MapReduce库会在一个集群的若干台机器上启动程序的多个副本。
  * 程序的各个副本中有一个是特殊的——主节点，其它的则是工作节点。主节点将M个map任务和R个reduce任务分配给空闲的工作节点，每个节点一项任务。
  * 被分配map任务的工作节点读取对应的输入区块内容。它从输入数据中解析出key/value对，然后将每个对传递给用户定义的map函数。由map函数产生的中间key/value对都缓存在内存中。
  * 缓存的数据对会被周期性的由划分函数分成R块，并写入本地磁盘中。这些缓存对在本地磁盘中的位置会被传回给主节点，主节点负责将这些位置再传给reduce工作节点。
  * 当一个reduce工作节点得到了主节点的这些位置通知后，它使用RPC调用去读map工作节点的本地磁盘中的缓存数据。当reduce工作节点读取完了所有的中间数据，它会将这些数据按中间key排序，这样相同key的数据就被排列在一起了。同一个reduce任务经常会分到有着不同key的数据，因此这个排序很有必要。如果中间数据数量过多，不能全部载入内存，则会使用外部排序。
  * reduce工作节点遍历排序好的中间数据，并将遇到的每个中间key和与它关联的一组中间value传递给用户的reduce函数。reduce函数的输出会写到由reduce划分过程划分出来的最终输出文件的末尾。
  * 当所有的map和reduce任务都完成后，主节点唤醒用户程序。此时，用户程序中的MapReduce调用返回到用户代码中。
  * 成功完成后，MapReduce执行的输出都在R个输出文件中（每个reduce任务产生一个，文件名由用户指定）。通常用户不需要合并这R个输出文件——他们经常会把这些文件当作另一个MapReduce调用的输入，或是用于另一个可以处理分成多个文件输入的分布式应用。
  * 如图：
    * ![执行流程](https://upload-images.jianshu.io/upload_images/1814354-e33da96f1211ef42.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000/format/webp) 

* 容错性：
  * worker节点：master会周期性的ping每个机器，如果一个worker在一定时间内没有回应，master会将该机器标记为failed。这时候，这个机器上的所有已完成map任务都会被标记为空闲状态，这样那些map任务就会被重新调度给其他worker去处理。同样，任何在一个失败机器正在执行的map或者reduce任务也会被重置为空闲状态等待重新调度。可能会有疑问，为什么map任务都完成了还需要重新执行，这是因为已完成map任务的输出是存储在失败机器的本地磁盘上，这时候这些数据在机器有问题以后是不能被访问的；而已完成的reduce则不需要重新执行是因为这些输出已经存到了最终输出文件里面去了。当一个map任务在A机器上执行失败后，在B机器上会重新执行，这时候，master会通知所有正在执行reduce任务的worker这种变更，这样还没有从A机器获取数据的reduce任务会从B机器上获取。
  * master节点：对于master的可能性失败，一种比较简单的方法是周期性的将前述master的数据结构写到磁盘里面去，这样万一master挂了，另一个master会从最后写入的数据中恢复出来。

* 任务的粒度：
    * 我们细分map阶段成M个片,reduce阶段成R个片.M和R应当比worker机器的数量大许多.每个worker执行许多不同的工作来提高动态负载均衡,也可以加速从一个worker失效中的恢复,这个机器上的许多已经完成的map任务可以被分配到所有其他的worker机器上.
    * 在我们的实现里,M和R的范围是有大小限制的,因为master必须做O(M+R)次调度,并且保存O(M*R)个状态在内存中.(这个因素使用的内存是很少的,在O(M*R)个状态片里,大约每个map任务/reduce任务对使用一个字节的数据).
    * 此外,R经常被用户限制,因为每一个reduce任务最终都是一个独立的输出文件.实际上,我们倾向于选择M,以便每一个单独的任务大概都是16到64MB的输入数据(以便上面描述的位置优化是最有效的),我们把R设置成我们希望使用的worker机器数量的小倍数.我们经常执行MapReduce计算,在M=200000,R=5000,使用2000台工作者机器的情况下.

* 备用任务：
    * 一个落后者是延长MapReduce操作时间的原因之一:一个机器花费一个异乎寻常地的长时间来完成最后的一些map或reduce任务中的一个.有很多原因可能产生落后者.例如,一个有坏磁盘的机器经常发生可以纠正的错误,这样就使读性能从30MB/s降低到3MB/s.机群调度系统也许已经安排其他的任务在这个机器上,由于计算要使用CPU,内存,本地磁盘,网络带宽的原因,引起它执行MapReduce代码很慢.我们最近遇到的一个问题是,一个在机器初始化时的Bug引起处理器缓存的失效:在一个被影响的机器上的计算性能有上百倍的影响.
    * 我们有一个一般的机制来减轻这个落后者的问题.当一个MapReduce操作将要完成的时候,master调度备用进程来执行那些剩下的还在执行的任务.无论是原来的还是备用的执行完成了,工作都被标记成完成.我们已经调整了这个机制,通常只会占用多几个百分点的机器资源.我们发现这可以显著的减少完成大规模MapReduce操作的时间.作为一个例子，在关闭掉备用任务的情况下,要比有备用任务的情况下多花44%的时间.